{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6166cae6-324e-4c93-9a93-b011b8b501f6",
   "metadata": {},
   "source": [
    "Apache Spark - it is a analytics engine for massive datasets, it uses distributed clusters to speed up analytics \n",
    "\n",
    "Spark is faster than other analytical engine because it reduce the execution of repetitive function by in-memory processing(using cache for faster processing)\n",
    "\n",
    "pySpark - pyspark is a derivation of  apache spark , which is written on python and allows users to use python to utilize the spark \n",
    "\n",
    "Advantages of pyspark:\n",
    "- In-memory computation\n",
    "- Cache utilization\n",
    "- Distributed processing\n",
    "- Supports SQL\n",
    "\n",
    "Architecture of PySpark:\n",
    "- Master-slave architecture\n",
    "- Master => Driver\n",
    "- Slave => Workers\n",
    "- Cluster Manager does the resource management job\n",
    "\n",
    "RDD(Resilient Distributed Dataset) - it is a basic data type of apache spark which stores massive datasets into various servers, which enables parallel execution.\n",
    "\n",
    "2 actions in RDD is:\n",
    "- Transformation - it produces a new RDD as an output, it is a lazy operation i.e without an execution of a new line the RDD is not created.    \n",
    "  Transformations can be wide or narrow\n",
    "- Actions - these are common executions that are used in analytics ex: first, count, sort…\n",
    "\n",
    "Spark also supports third party libraries/modules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902aa341-daa6-4e0a-90c7-803ca0a5e866",
   "metadata": {},
   "source": [
    "## Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8acda19-e82c-4527-8b42-f9ba00c2ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PysparkPractice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7af7c9a-3e09-485b-9649-823dc107b195",
   "metadata": {},
   "source": [
    "### creating dataframe and basic df operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8d3a648-aa3b-4d23-be64-d54a2d7687a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+\n",
      "|   name| age| exp|salary|\n",
      "+-------+----+----+------+\n",
      "|karthik|  20|   1| 10000|\n",
      "|  gokul|  35|  10|  NULL|\n",
      "|   subu|  25|   4| 20000|\n",
      "|   ram |NULL|   1| 10000|\n",
      "| muthu |  45|  20| 80000|\n",
      "|kishore|  32|  10| 25000|\n",
      "|   john|  27|NULL| 15000|\n",
      "|aravind|  34|  10| 25000|\n",
      "|   arun|  26|   5| 10000|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"test.csv\", header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb448a4-7969-466c-a3dc-b588bea37651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)      #instead of pandas.core dataframe it is a sql dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dfad3a-1383-4773-905f-d4af0f593c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04852c70-7780-4861-ae81-75c230129621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'exp', 'salary']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed9c4461-4675-4cb4-8fc5-00213417cabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|karthik|\n",
      "|  gokul|\n",
      "|   subu|\n",
      "|   ram |\n",
      "| muthu |\n",
      "|kishore|\n",
      "|   john|\n",
      "|aravind|\n",
      "|   arun|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name').show()        #to select a perticular column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64374e03-dc55-43ad-b6d9-ccdedf9ad4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+\n",
      "|   name| age|\n",
      "+-------+----+\n",
      "|karthik|  20|\n",
      "|  gokul|  35|\n",
      "|   subu|  25|\n",
      "|   ram |NULL|\n",
      "| muthu |  45|\n",
      "|kishore|  32|\n",
      "|   john|  27|\n",
      "|aravind|  34|\n",
      "|   arun|  26|\n",
      "+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('name', 'age').show()    #for multiple colunms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c839a1a-939a-405a-908d-e7d9a84ecf1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('exp', 'int'), ('salary', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#slicing is not supported in pyspark\n",
    "df.dtypes   #show data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3f86e2-81a8-4221-8154-4c0b4969dfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+-----------------+-----------------+\n",
      "|summary|   name|              age|              exp|           salary|\n",
      "+-------+-------+-----------------+-----------------+-----------------+\n",
      "|  count|      9|                8|                8|                8|\n",
      "|   mean|   NULL|             30.5|            7.625|          24375.0|\n",
      "| stddev|   NULL|7.727501906456298|6.300510183423925|23366.26078038895|\n",
      "|    min|aravind|               20|                1|            10000|\n",
      "|    max|   subu|               45|               20|            80000|\n",
      "+-------+-------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()  #describe the info about dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5cb97a-921e-4724-bb41-ccb2ef876df3",
   "metadata": {},
   "source": [
    "###  Ways to Rename column on DataFrame\n",
    "\n",
    "pyspark uses __withColumnRenamed__ method to rename column, hence dataframe is immutable it returns a new DataFrame with columns renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec39517-7aa0-4cda-b9f7-8dcbdb77ebe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+------+\n",
      "|   name| age|experience|salary|\n",
      "+-------+----+----------+------+\n",
      "|karthik|  20|         1| 10000|\n",
      "|  gokul|  35|        10|  NULL|\n",
      "|   subu|  25|         4| 20000|\n",
      "|   ram |NULL|         1| 10000|\n",
      "| muthu |  45|        20| 80000|\n",
      "|kishore|  32|        10| 25000|\n",
      "|   john|  27|      NULL| 15000|\n",
      "|aravind|  34|        10| 25000|\n",
      "|   arun|  26|         5| 10000|\n",
      "+-------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumnRenamed('exp','experience').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec878d9-55c6-4d14-9cb5-b8eb46ce8531",
   "metadata": {},
   "source": [
    "### withColumn() \n",
    "\n",
    "this is a transformation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642cc9e0-f088-4081-917b-0a8f348d7024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "788c9316-b174-4e18-ae86-073fed2b90fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#changing DataType using withColunm()\n",
    "df.withColumn('exp',df['exp'].cast(\"String\")).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c5a3584-f5cf-4feb-b78e-468fe939416d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='karthik', age=20, exp=1, salary=120000),\n",
       " Row(name='gokul', age=35, exp=10, salary=None),\n",
       " Row(name='subu', age=25, exp=4, salary=240000),\n",
       " Row(name='ram ', age=None, exp=1, salary=120000),\n",
       " Row(name='muthu ', age=45, exp=20, salary=960000)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# updating value of the column\n",
    "df.withColumn('salary',df['salary']*12).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c62fc70-2fce-4381-b648-d594c81bc4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='karthik', age=20, exp=1, salary=10000, exp after 5 yrs=6),\n",
       " Row(name='gokul', age=35, exp=10, salary=None, exp after 5 yrs=15),\n",
       " Row(name='subu', age=25, exp=4, salary=20000, exp after 5 yrs=9),\n",
       " Row(name='ram ', age=None, exp=1, salary=10000, exp after 5 yrs=6),\n",
       " Row(name='muthu ', age=45, exp=20, salary=80000, exp after 5 yrs=25)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new column\n",
    "df.withColumn('exp after 5 yrs', df['exp']+5).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7fc5614-7eb8-42f2-90dd-8bf6837b4612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='karthik', age=20, salary=10000),\n",
       " Row(name='gokul', age=35, salary=None),\n",
       " Row(name='subu', age=25, salary=20000),\n",
       " Row(name='ram ', age=None, salary=10000),\n",
       " Row(name='muthu ', age=45, salary=80000)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping columns\n",
    "df.drop('exp').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6399b3e0-c8f3-4569-8158-0254982a2c73",
   "metadata": {},
   "source": [
    "### filter()\n",
    "\n",
    "its alternative is where(), both the method supports SQL experssions to filter the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e23b977-72c3-45a2-8840-48444fbfe896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|   name|age|exp|salary|\n",
      "+-------+---+---+------+\n",
      "|  gokul| 35| 10|  NULL|\n",
      "| muthu | 45| 20| 80000|\n",
      "|kishore| 32| 10| 25000|\n",
      "|aravind| 34| 10| 25000|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['exp'] > 5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcc96543-3d76-46cc-8583-9e5b77da88a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|   name|age|exp|salary|\n",
      "+-------+---+---+------+\n",
      "|  gokul| 35| 10|  NULL|\n",
      "|kishore| 32| 10| 25000|\n",
      "|aravind| 34| 10| 25000|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter((df.exp > 5) & (df.exp < 15)).show() #multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195c71ba-f3fc-4e99-8d6c-a73d0e1c21bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|   name|age|exp|salary|\n",
      "+-------+---+---+------+\n",
      "|  gokul| 35| 10|  NULL|\n",
      "|aravind| 34| 10| 25000|\n",
      "|   arun| 26|  5| 10000|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = [\"gokul\", \"arun\", \"aravind\"]\n",
    "df.filter(df['name'].isin(names)).show()  #isin() method allows to filter from a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1df292e0-aa10-41c4-ab0e-9ce1863ef4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|   name|age|exp|salary|\n",
      "+-------+---+---+------+\n",
      "|aravind| 34| 10| 25000|\n",
      "|   arun| 26|  5| 10000|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['name'].startswith('a')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12fcc645-241e-4f9f-864f-2ffe727f3114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+---+------+\n",
      "|   name| age|exp|salary|\n",
      "+-------+----+---+------+\n",
      "|karthik|  20|  1| 10000|\n",
      "|   ram |NULL|  1| 10000|\n",
      "|aravind|  34| 10| 25000|\n",
      "|   arun|  26|  5| 10000|\n",
      "+-------+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['name'].contains('a')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21184720-70dd-4720-9ec6-958619f657b6",
   "metadata": {},
   "source": [
    "### orderby() & sort()\n",
    "\n",
    "orderby() is the alternative/alias of sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1aaad671-44ae-4270-b509-7dde74764285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+\n",
      "|   name| age| exp|salary|\n",
      "+-------+----+----+------+\n",
      "|aravind|  34|  10| 25000|\n",
      "|   arun|  26|   5| 10000|\n",
      "|  gokul|  35|  10|  NULL|\n",
      "|   john|  27|NULL| 15000|\n",
      "|karthik|  20|   1| 10000|\n",
      "|kishore|  32|  10| 25000|\n",
      "| muthu |  45|  20| 80000|\n",
      "|   ram |NULL|   1| 10000|\n",
      "|   subu|  25|   4| 20000|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f2b45a8-762b-4e38-bac2-8f0c375bb2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+\n",
      "|   name| age| exp|salary|\n",
      "+-------+----+----+------+\n",
      "|   subu|  25|   4| 20000|\n",
      "|   ram |NULL|   1| 10000|\n",
      "| muthu |  45|  20| 80000|\n",
      "|kishore|  32|  10| 25000|\n",
      "|karthik|  20|   1| 10000|\n",
      "|   john|  27|NULL| 15000|\n",
      "|  gokul|  35|  10|  NULL|\n",
      "|   arun|  26|   5| 10000|\n",
      "|aravind|  34|  10| 25000|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.sort(df['name'].desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60be0a-5a50-436f-890b-d7deceeebd28",
   "metadata": {},
   "source": [
    "### Groupby()\n",
    "\n",
    "it is similar to SQL GROUP BY, it returns GroupedData object\n",
    "\n",
    "* count()\n",
    "* mean()\n",
    "* max\n",
    "* min()\n",
    "* sum()\n",
    "* avg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a7870dee-acc1-4988-a2fb-3fd639a3f143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "| exp|sum(salary)|\n",
      "+----+-----------+\n",
      "|NULL|      15000|\n",
      "|   1|      20000|\n",
      "|  20|      80000|\n",
      "|   5|      10000|\n",
      "|   4|      20000|\n",
      "|  10|      50000|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('exp').sum('salary').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c9caef31-67a0-49a0-8776-5fdec5139da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| exp|count|\n",
      "+----+-----+\n",
      "|NULL|    1|\n",
      "|   1|    2|\n",
      "|  20|    1|\n",
      "|   5|    1|\n",
      "|   4|    1|\n",
      "|  10|    3|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('exp').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bfd0c9a9-5186-4364-aa64-19fe257db38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|exp|count|\n",
      "+---+-----+\n",
      "| 20|    1|\n",
      "| 10|    3|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('exp').count().filter(df['exp'] > 5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65d18d-d83e-4da5-b714-c266169a02da",
   "metadata": {},
   "source": [
    "### SQL IN PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ae19ba03-cea5-4b07-8347-109f6bd2290f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- exp: integer (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+-------+----+----+------+\n",
      "|   name| age| exp|salary|\n",
      "+-------+----+----+------+\n",
      "|karthik|  20|   1| 10000|\n",
      "|  gokul|  35|  10|  NULL|\n",
      "|   subu|  25|   4| 20000|\n",
      "|   ram |NULL|   1| 10000|\n",
      "| muthu |  45|  20| 80000|\n",
      "|kishore|  32|  10| 25000|\n",
      "|   john|  27|NULL| 15000|\n",
      "|aravind|  34|  10| 25000|\n",
      "|   arun|  26|   5| 10000|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"PERSON_DATA\")\n",
    "df2 = spark.sql(\"SELECT * from PERSON_DATA\")\n",
    "df2.printSchema()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50103b0-7b9e-4865-9a98-cf777eaf629d",
   "metadata": {},
   "source": [
    "### when().otherwise()\n",
    "\n",
    "this is similar to switch case in other programming language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "04ddc29d-0947-4120-99f7-cb4b17eb1108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+--------------+\n",
      "|   name| age| exp|salary|classification|\n",
      "+-------+----+----+------+--------------+\n",
      "|karthik|  20|   1| 10000|  less than 30|\n",
      "|  gokul|  35|  10|  NULL|  less than 40|\n",
      "|   subu|  25|   4| 20000|  less than 30|\n",
      "|   ram |NULL|   1| 10000|          NULL|\n",
      "| muthu |  45|  20| 80000|            45|\n",
      "|kishore|  32|  10| 25000|  less than 40|\n",
      "|   john|  27|NULL| 15000|  less than 30|\n",
      "|aravind|  34|  10| 25000|  less than 40|\n",
      "|   arun|  26|   5| 10000|  less than 30|\n",
      "+-------+----+----+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.withColumn(\"classification\", \n",
    "               when(df['age'] < 30, \"less than 30\")\n",
    "              .when(df['age'] < 40, \"less than 40\")\n",
    "              .otherwise(df['age'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec18d50-cc3e-4f18-9f2e-e215936a9e9a",
   "metadata": {},
   "source": [
    "#### Window function\n",
    "\n",
    "these functions allow us to rank, allocate row number to the rows of dataframe based on given coloumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "07e527a5-5826-4ce9-98b4-f3105498de8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+----------+\n",
      "|   name| age| exp|salary|row_number|\n",
      "+-------+----+----+------+----------+\n",
      "|  gokul|  35|  10|  NULL|         1|\n",
      "|karthik|  20|   1| 10000|         2|\n",
      "|   ram |NULL|   1| 10000|         3|\n",
      "|   arun|  26|   5| 10000|         4|\n",
      "|   john|  27|NULL| 15000|         5|\n",
      "|   subu|  25|   4| 20000|         6|\n",
      "|kishore|  32|  10| 25000|         7|\n",
      "|aravind|  34|  10| 25000|         8|\n",
      "| muthu |  45|  20| 80000|         9|\n",
      "+-------+----+----+------+----------+\n",
      "\n",
      "+-------+----+----+------+----+\n",
      "|   name| age| exp|salary|rank|\n",
      "+-------+----+----+------+----+\n",
      "|  gokul|  35|  10|  NULL|   1|\n",
      "|karthik|  20|   1| 10000|   2|\n",
      "|   ram |NULL|   1| 10000|   2|\n",
      "|   arun|  26|   5| 10000|   2|\n",
      "|   john|  27|NULL| 15000|   5|\n",
      "|   subu|  25|   4| 20000|   6|\n",
      "|kishore|  32|  10| 25000|   7|\n",
      "|aravind|  34|  10| 25000|   7|\n",
      "| muthu |  45|  20| 80000|   9|\n",
      "+-------+----+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, rank\n",
    "windowSpec = Window.orderBy(\"salary\")\n",
    "\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show()\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7279c62-87dd-409a-9c2e-9a85361769d8",
   "metadata": {},
   "source": [
    "#### Handling Missing Values \n",
    "\n",
    "* we can drop rows with null value\n",
    "* we can fill the null values with 0, empty string or mean/mode/median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "610a96a0-14c0-439a-b2f8-c6bc90c17501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|   name|age|exp|salary|\n",
      "+-------+---+---+------+\n",
      "|karthik| 20|  1| 10000|\n",
      "|   subu| 25|  4| 20000|\n",
      "| muthu | 45| 20| 80000|\n",
      "|kishore| 32| 10| 25000|\n",
      "|aravind| 34| 10| 25000|\n",
      "|   arun| 26|  5| 10000|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1d9e4de7-5d7d-4241-b39c-d0f3b55f04be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+------+\n",
      "|   name|age|exp|salary|\n",
      "+-------+---+---+------+\n",
      "|karthik| 20|  1| 10000|\n",
      "|  gokul| 35| 10|     0|\n",
      "|   subu| 25|  4| 20000|\n",
      "|   ram |  0|  1| 10000|\n",
      "| muthu | 45| 20| 80000|\n",
      "|kishore| 32| 10| 25000|\n",
      "|   john| 27|  0| 15000|\n",
      "|aravind| 34| 10| 25000|\n",
      "|   arun| 26|  5| 10000|\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.fill(0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3c1332-8618-49d4-9cd9-f39d5e88c77e",
   "metadata": {},
   "source": [
    "#### PySpark UDF(User Defined Function)\n",
    "\n",
    "PySpark UDF’s are similar to UDF on traditional databases. . UDF’s are once created they can be re-used on several DataFrames\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "07d4a01d-2917-47db-9c48-eb477d199928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalizeString(string):\n",
    "    return string.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "450f0775-8d6e-4fb9-a802-53af5fff5ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "convertUDF = udf(lambda z: capitalizeString(z),StringType())\n",
    "# df.withColumn(\"newName\",convertUDF(col('name'))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd245bd-8247-41ac-b1cc-35598fb90be9",
   "metadata": {},
   "source": [
    "#### isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2e8dc3ef-82c6-4925-bcf5-44d6e99582c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---+------+\n",
      "|name| age|exp|salary|\n",
      "+----+----+---+------+\n",
      "|ram |NULL|  1| 10000|\n",
      "+----+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['age'].isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf49e9-2733-4e26-9d5b-17c68dea9e57",
   "metadata": {},
   "source": [
    "#### Concat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "594a15f4-a6d6-4a88-9f1f-13966b5887f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|concat_ws(_, name, age)|\n",
      "+-----------------------+\n",
      "|             karthik_20|\n",
      "|               gokul_35|\n",
      "|                subu_25|\n",
      "|                   ram |\n",
      "|              muthu _45|\n",
      "|             kishore_32|\n",
      "|                john_27|\n",
      "|             aravind_34|\n",
      "|                arun_26|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import concat_ws\n",
    "df.select(concat_ws('_',df['name'],df['age'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e1ae1f-a4c9-40b3-9e7b-f54e667112c1",
   "metadata": {},
   "source": [
    "#### Pyspark Select Distinct Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "f569b7fd-5be5-4d75-a9fa-1d65a8362e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+\n",
      "|   name| age| exp|salary|\n",
      "+-------+----+----+------+\n",
      "| muthu |  45|  20| 80000|\n",
      "|karthik|  20|   1| 10000|\n",
      "|   arun|  26|   5| 10000|\n",
      "|   subu|  25|   4| 20000|\n",
      "|kishore|  32|  10| 25000|\n",
      "|aravind|  34|  10| 25000|\n",
      "|   john|  27|NULL| 15000|\n",
      "|  gokul|  35|  10|  NULL|\n",
      "|   ram |NULL|   1| 10000|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c8962-2533-475c-b7e1-21c0528eddf9",
   "metadata": {},
   "source": [
    "#### Colunm and row Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b40cab79-97a1-4f2b-a611-30c3ccde1157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row =  9 col =  4\n"
     ]
    }
   ],
   "source": [
    "print(\"row = \", df.count(), \"col = \", len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e9306a-a54a-408b-830c-dd035b49013b",
   "metadata": {},
   "source": [
    "#### union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fc523bf-5759-4cdc-85c2-a8fa5258b8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- exp: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------+---+---+------+\n",
      "|name   |exp|age|salary|\n",
      "+-------+---+---+------+\n",
      "|James  |9  |34 |10000 |\n",
      "|Michael|18 |56 |20000 |\n",
      "+-------+---+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleData = [(\"James\",9,34,10000),(\"Michael\",18,56,20000)]\n",
    "\n",
    "columns= [\"name\",\"exp\",\"age\",\"salary\"]\n",
    "df2 = spark.createDataFrame(data = simpleData, schema = columns)\n",
    "df2.printSchema()\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a37ea77-7748-498c-80a7-8922809c7e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----+------+\n",
      "|   name| age| exp|salary|\n",
      "+-------+----+----+------+\n",
      "|karthik|  20|   1| 10000|\n",
      "|  gokul|  35|  10|  NULL|\n",
      "|   subu|  25|   4| 20000|\n",
      "|   ram |NULL|   1| 10000|\n",
      "| muthu |  45|  20| 80000|\n",
      "|kishore|  32|  10| 25000|\n",
      "|   john|  27|NULL| 15000|\n",
      "|aravind|  34|  10| 25000|\n",
      "|   arun|  26|   5| 10000|\n",
      "|  James|   9|  34| 10000|\n",
      "|Michael|  18|  56| 20000|\n",
      "+-------+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc03a698-1d2b-4e8d-a64b-73ea69d5c216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
